# llm-safety-jailbreak-eval-analysis
A practical AI safety study combining manual and automated jailbreaks (GCG, AutoDAN) with layered defenses including self-reminders, hierarchical prompting, guard models, and perplexity filters.
